# Deep_Learning_Project

Abstract: 

In the field of natural language processing (NLP), text classification plays a crucial role, extending to areas like sentiment analysis, spam detection, and topic categorization. This research delves into evaluating Transformer-based models for text classification, aiming to identify the most effective among prominent models such as BERT, ELECTRA, RoBERTa, distilBERT, and ALBERT. 

The study uses a comprehensive set of metrics, including accuracy, precision, recall, F1-score, and AUC-ROC, to evaluate model performance on a Twitter emotion dataset. This dataset choice provides a robust basis for evaluation across different classification contexts. Additionally, the research investigates how factors like model size, scale of training data, and pretraining strategies influence model performance, shedding light on the relationship between architectural complexity and training resources. This study's findings will offer insights into optimizing text classification models in various NLP applications.
